primary_name: "Shuzheng Si"
secondary_name: ""
navbar_name: "Shuzheng Si's Home Page"

positions:
  - '<span class="no-break"><img src="assets/images/badges/tsinghua.png" alt="Tsinghua University" class="inline-badge"/></span> Ph.D. student, Tsinghua University'

email: "ssz24@mails.tsinghua.edu.cn"
gscholar: zO2XyZUAAAAJ
# sscholar: Rongsheng-Wang/2261313297
github: S1s-Z
# twitter: RongshengWang
# wechat_qrcode: assets/images/etc/wechat.jpg
# wechat_prompt: >-
#   Please tell me your <strong>name</strong> and <strong>affiliation</strong> (current or past) when adding my wechat. Thanks!
# zhihu: wang-rong-sheng-74
# resume: assets/cv/Resume_RongshengWang.pdf
# linkedin: your-linked-in-id
# orcid: 0000-0000-0000-0000
# xiaohongshu: 5fa2663000000000010068e2

short_bio: >-
  Iâ€™m Shuzheng Si, a first-year Ph.D. student in the Department of Computer Science and Technology at Tsinghua University. I am lucky to be advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=zIgT0HMAAAAJ">Maosong Sun</a> and affiliated with <a href="https://nlp.csai.tsinghua.edu.cn">TsinghuaNLP Lab</a>. Previously, I obtained my masterâ€™s degree from Peking University, where I was fortunate to be a part of the <a href="https://pkunlp-icler.github.io/">PKU NLP Group</a> under the supervision of Prof. <a href="https://scholar.google.com.au/citations?user=LaKNyhQAAAAJ">Baobao Chang</a> at the Institute of Computational Linguistics. My research interests lie in <b>Natural Language Processing (NLP) and Large Language Models (LLMs)</b>, specifically focusing on <b>Data-Centric Methods and Data Science for NLP</b>, including:
  <br>
  <ul class="pub">
      <li>
          ðŸ’¡ <b style="text-decoration: underline;">Data Synthesis & Data Selection (Better Understanding of Data)</b>: My first line of research aims to synthesize data more scientifically (<a href="https://arxiv.org/abs/2505.16483">CANOE</a>, <a href="https://arxiv.org/abs/2502.04153">UltraIF</a>, <a href="https://arxiv.org/abs/2407.05282">UltraEdit</a>) and to automatically and effectively evaluate data quality (<a href="https://arxiv.org/abs/2502.07340">NOVA</a>, <a href="https://arxiv.org/abs/2410.15633">GATEAU</a>, <a href="https://arxiv.org/abs/2312.10302">NUGGETS</a>), thereby further advancing the capabilities of LLMs. At the same time, I believe that by designing better data synthesis and data selection methods, we can gain deeper insights into the role of data in LLM training and ultimately make more efficient use of data.
      </li>
      <li>
          ðŸš€ <b style="text-decoration: underline;">Data-Efficient Methods for Post-Training (Better Utilization of Data)</b>: Current LLMs require training on a vast amount of data, which significantly increases the training costs of models. This line of research attempts to utilize data more efficiently, such as leveraging (noisy) data collected from real-world scenarios to train models (<a href="https://arxiv.org/abs/2305.04076">SANTA</a>, <a href="https://arxiv.org/abs/2209.01646">SCL-RAI</a>, <a href="https://arxiv.org/abs/2305.13040">SpokenWOZ</a>, <a href="https://arxiv.org/abs/2311.08010">CENSOR</a>), and maximizing the positive impact of the limited data on models (<a href="https://arxiv.org/abs/2309.07915">MMICL</a>, <a href="https://arxiv.org/abs/2404.08491">ALSACE</a>, <a href="https://arxiv.org/abs/2411.14279">LACING</a>). Ultimately, the goal is to achieve data-efficient artificial intelligence and reach the next level of intelligence (perhaps referred to as AGI) at minimal cost..
      </li>
  </ul>

  My long-term research goal is to elucidate the influence of data on LLMs and subsequently utilize these insights to effectively guide the organization, selection, and synthesis of high-quality data, thereby enhancing the foundational capabilities of LLMs. 
  Recently, I have been very interested in investigating LLMs' hallucinations from a data perspective and attempting to mitigate hallucinations through data-driven approaches.
  <br>
  I'm also interested in working together, so please feel free to email me if there are any opportunities!





show_portrait: False
portrait_url: assets/images/photos/ssz.jpg
portrait_caption: >-
  Welcome to my homepage! This photo was taken before I started my research in NLP. Compared to back then, I've gained quite a bit of weight and lost quite a bit of hair :(

education:
- name: Tsinghua University
  logo: assets/images/badges/tsinghua.png
  position: Ph.D. in Computer Science and Technology
  date: Sep. 2024 - Jul. 2028 (expected)
- name: Peking University 
  logo: assets/images/badges/PKU_red.png
  position: M.S. in Software Engineering
  date: Sep. 2021 - Jul. 2024
- name: Yunnan University 
  logo: assets/images/badges/ynu.png
  position: "B.S. in Information Security (rank: 1/300+)"
  date: Sep. 2017 - Jul. 2021
  

experience:
- name: DeepLang AI
  logo: assets/images/badges/deeplang.png
  position: Research Intern
  date: Apr. 2024 - Now
- name: Alibaba DAMO Academy
  logo: assets/images/badges/ali.png
  position: Research Intern
  date: Jun. 2022 - Jun. 2023
- name: SenseTime Research
  logo: assets/images/badges/sensetime.png
  position: Research Intern
  date: Jul. 2021 -	Feb. 2022


awards:
- name: Merit Student
  date: 2022
- name: "Top 10 Outstanding Students Award (Rank #1 in Nomination Awards)"
  date: 2020
- name: First Class Scholarship
  date: 2020
- name: National Scholarship
  date: 2019
- name: Provincial Scholarship
  date: 2018
- name: Provincial Merit Student
  date: 2018
  
service:
- name: Reviewer of ACL, EMNLP, NAACL, and COLING
- name: Reviewer of NeurIPS, ICLR, and ICML
- name: Reviewer of ICCV
- name: Reviewer of TASLP

