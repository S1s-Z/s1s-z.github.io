primary_name: "Shuzheng Si"
secondary_name: ""
navbar_name: "Shuzheng Si's Home Page"

positions:
  - '<span class="no-break"><img src="assets/images/badges/tsinghua.png" alt="Tsinghua University" class="inline-badge"/></span> Ph.D. student, Tsinghua University'

email: "ssz24@mails.tsinghua.edu.cn"
gscholar: zO2XyZUAAAAJ
# sscholar: Rongsheng-Wang/2261313297
github: S1s-Z
# twitter: RongshengWang
# wechat_qrcode: assets/images/etc/wechat.jpg
# wechat_prompt: >-
#   Please tell me your <strong>name</strong> and <strong>affiliation</strong> (current or past) when adding my wechat. Thanks!
# zhihu: wang-rong-sheng-74
# resume: assets/cv/Resume_RongshengWang.pdf
# linkedin: your-linked-in-id
# orcid: 0000-0000-0000-0000
# xiaohongshu: 5fa2663000000000010068e2

short_bio: >-
  Hi, I‚Äôm Shuzheng Si, a first-year Ph.D. student in the Department of Computer Science and Technology at Tsinghua University. I am lucky to be advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=zIgT0HMAAAAJ">Maosong Sun</a> and affiliated with <a href="https://nlp.csai.tsinghua.edu.cn">TsinghuaNLP Lab</a>. Previously, I obtained my master‚Äôs degree from Peking University, where I was fortunate to be a part of the <a href="https://pkunlp-icler.github.io/">PKU NLP Group</a> under the supervision of Prof. <a href="https://scholar.google.com.au/citations?user=LaKNyhQAAAAJ">Baobao Chang</a> at the Institute of Computational Linguistics. My research interests lie in <b>Natural Language Processing (NLP) and Large Language Models (LLMs)</b>, specifically focusing on <b>Data-Centric Methods and Data Science for NLP</b>, including:
  <br>
  <ul class="pub">
      <li>
          üí° <b>Data Synthesis & Data Selection for Post-Training (üïµüèª Better Understanding of Data)</b>: My first line of research aims to synthesize data more scientifically (<a href="https://arxiv.org/abs/2505.16483">CANOE</a>, <a href="https://arxiv.org/abs/2502.04153">UltraIF</a>, <a href="https://arxiv.org/abs/2407.05282">UltraEdit</a>), as well as automatically ensure the data quality (<a href="https://arxiv.org/abs/2502.07340">NOVA</a>, <a href="https://arxiv.org/abs/2410.15633">GATEAU</a>, <a href="https://arxiv.org/abs/2312.10302">NUGGETS</a>), thereby further advancing the capabilities of LLMs. At the same time, I believe that by designing better data synthesis and data selection methods, we can gain deeper insights into the role of data in LLM training and ultimately make more efficient use of data.
      </li>
      <li>
          üöÄ <b>Data-Efficient Tuning Methods for Post-Training (üßëüèª‚Äçüî¨ Better Utilization of Data)</b>: Current LLMs require training on a vast amount of data, which significantly increases the training costs of models. This line of research attempts to utilize data more efficiently, such as leveraging (noisy) data collected from real-world scenarios to train models (<a href="https://arxiv.org/abs/2305.04076">SANTA</a>, <a href="https://arxiv.org/abs/2209.01646">SCL-RAI</a>, <a href="https://arxiv.org/abs/2305.13040">SpokenWOZ</a>, <a href="https://arxiv.org/abs/2311.08010">CENSOR</a>), and maximizing the positive impact of the limited data on models (<a href="https://arxiv.org/abs/2309.07915">MMICL</a>, <a href="https://arxiv.org/abs/2404.08491">ALSACE</a>, <a href="https://arxiv.org/abs/2411.14279">LACING</a>). Ultimately, the goal is to achieve data-efficient artificial intelligence and reach the next level of intelligence (perhaps referred to as AGI) at minimal cost.
      </li>
  </ul>

  My long-term research goal is to elucidate the influence of data on LLMs and utilize these insights to effectively guide the organization, selection, and synthesis of high-quality data, thereby enhancing the foundational capabilities of LLMs. 
  Recently, I have been very interested in investigating LLMs' hallucinations from a data perspective and attempting to mitigate hallucinations through well-designed data-driven methods.
  <br>
  <br>
  Feel free to drop an email if you are interested in connecting. üïäüïäüïä





show_portrait: False
portrait_url: assets/images/photos/ssz.jpg
portrait_caption: >-
  Welcome to my homepage! This photo was taken before I started my research in NLP. Compared to back then, I've gained quite a bit of weight and lost quite a bit of hair :(

education:
- name: Tsinghua University
  logo: assets/images/badges/tsinghua.png
  position: Ph.D. in Computer Science and Technology
  date: Sep. 2024 - Jul. 2028 (expected)
- name: Peking University 
  logo: assets/images/badges/PKU_red.png
  position: M.S. in Software Engineering
  date: Sep. 2021 - Jul. 2024
- name: Yunnan University 
  logo: assets/images/badges/ynu.png
  position: "B.S. in Information Security (Rank: 1/300+)"
  date: Sep. 2017 - Jul. 2021
  

experience:
- name: DeepLang AI
  logo: assets/images/badges/deeplang.png
  position: Research Intern
  date: Apr. 2024 - Now
- name: Alibaba DAMO Academy
  logo: assets/images/badges/ali.png
  position: Research Intern
  date: Jun. 2022 - Jun. 2023
- name: SenseTime Research
  logo: assets/images/badges/sensetime.png
  position: Research Intern
  date: Jul. 2021 -	Feb. 2022


awards:
- name: Merit Student
  date: 2022
- name: "Top 10 Outstanding Students Nomination Award (Ranked 1st)"
  date: 2020
- name: First-Class Scholarship
  date: 2020
- name: National Scholarship
  date: 2019
- name: Provincial Scholarship
  date: 2018
- name: Provincial Merit Student
  date: 2018
  
service:
- name: "NLP Research Communities: Reviewer of ACL, EMNLP, NAACL, COLING, and TASLP"
- name: "ML Research Communities: Reviewer of NeurIPS, ICLR, and ICML"
- name: "CV Research Communities: Reviewer of ICCV"
- name: I am also a member of the <a href="https://huggingface.co/birdsql">BIRD team</a>, led by the talent researcher Jinyang Li, which drives the development of text-to-SQL for real-world database applications

