primary_name: "Shuzheng Si"
secondary_name: ""
navbar_name: "Shuzheng Si's Homepage üëãüèª"

positions:
  - '<span class="no-break"><img src="assets/images/badges/tsinghua.png" alt="Tsinghua University" class="inline-badge"/></span> Ph.D. student, Tsinghua University'

email: "ssz24@mails.tsinghua.edu.cn"
gscholar: zO2XyZUAAAAJ
sscholar: Shuzheng-Si/2053739525
github: S1s-Z
# twitter: RongshengWang
# wechat_qrcode: assets/images/etc/wechat.jpg
# wechat_prompt: >-
#   Please tell me your <strong>name</strong> and <strong>affiliation</strong> (current or past) when adding my wechat. Thanks!
# zhihu: wang-rong-sheng-74
# resume: assets/cv/Resume_RongshengWang.pdf
# linkedin: your-linked-in-id
# orcid: 0000-0000-0000-0000
# xiaohongshu: 5fa2663000000000010068e2

short_bio: >-
  Hi, I‚Äôm Shuzheng Si, a first-year Ph.D. student in the Department of Computer Science and Technology at Tsinghua University. I am lucky to be advised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=zIgT0HMAAAAJ">Maosong Sun</a> and affiliated with <a href="https://nlp.csai.tsinghua.edu.cn">TsinghuaNLP Lab</a>. Previously, I obtained my master‚Äôs degree from Peking University, where I was fortunate to be a part of the <a href="https://pkunlp-icler.github.io/">PKU NLP Group</a> under the supervision of Prof. <a href="https://scholar.google.com.au/citations?user=LaKNyhQAAAAJ">Baobao Chang</a> at the Institute of Computational Linguistics. My research interests lie in <b>Natural Language Processing (NLP) and Large Language Models (LLMs)</b>, specifically focusing on <b>Data-Centric Methods and Data Science for NLP</b>, including:
  <br>
  <ul class="pub">
      <li>
          üí° <b>Scientific Data Acquisition for Post-Training (üïµüèª Better Understanding of Data)</b>: My first line of research aims to synthesize data more scientifically (<a href="https://arxiv.org/abs/2505.16483">CANOE</a>, <a href="https://arxiv.org/abs/2502.04153">UltraIF</a>, <a href="https://arxiv.org/abs/2407.05282">UltraEdit</a>) and ensure the data quality automatically (<a href="https://arxiv.org/abs/2502.07340">NOVA</a>, <a href="https://arxiv.org/abs/2410.15633">GATEAU</a>, <a href="https://arxiv.org/abs/2312.10302">NUGGETS</a>), thereby further advancing the capabilities of LLMs. Also, by designing more effective data synthesis and data selection methods, we can gain deeper insights into the role of data in LLM training (e.g., when enhancing a LLM's specific capabilities, what characteristics should the selected data possess to be considered high-quality data in order to maximize data efficiency?), and ultimately make more efficient use of the data.
      </li>
      <li>
          üöÄ <b>Data-Efficient Tuning Methods for Post-Training (üßëüèª‚Äçüî¨ Better Utilization of Data)</b>: Current LLMs require training on a vast amount of data, which significantly increases the training costs of models. This line of research attempts to utilize the supervision derived from training data more efficiently, such as leveraging (noisy) data collected from real-world scenarios to train models (<a href="https://arxiv.org/abs/2305.13040">SpokenWOZ</a>, <a href="https://arxiv.org/abs/2305.04076">SANTA</a>, <a href="https://arxiv.org/abs/2311.08010">CENSOR</a>, <a href="https://arxiv.org/abs/2209.01646">SCL-RAI</a>), and maximizing the positive impact of the limited data on models (<a href="https://arxiv.org/abs/2309.07915">MMICL</a>, <a href="https://arxiv.org/abs/2404.08491">ALSACE</a>, <a href="https://arxiv.org/abs/2411.14279">LACING</a>). Ultimately, the goal is to achieve data-efficient artificial intelligence and reach the next level of intelligence (perhaps referred to as AGI) at minimal cost.
      </li>
      <li>
          üåè <b>Trustworthy and Helpful Language Processing Engine (üßëüèª‚Äçüè´ Applying My Research to Real-World Consumer Products)</b>: I also spend some time applying my research to the development of <a href="https://lingowhale.com/">LingoWhale</a>, which was created by DeepLang AI, a Chinese startup incubated by TsinghuaNLP Lab. LingoWhale attempts to build the next-generation language processing engine and help users access more valuable information in less time through subscription, aggregation, and summarization (More details are shown in this Chinese <a href="https://mp.weixin.qq.com/s/oB054vVcEBgD21VGZIqmNg">blog</a> ). Exceptionally low hallucination rates characterize it, ensuring an accurate and comprehensive representation of every viewpoint from the original text. To date, it has provided intelligent text information processing services to hundreds of thousands of Chinese users.
      </li>
  </ul>

  My long-term research goal is to elucidate the influence of data on LLMs and utilize these insights to guide the organization, selection, and synthesis of high-quality data, thereby enhancing the foundational capabilities of LLMs to build the next-generation language processing engine. 
  <b>Recently, I have been very interested in investigating LLMs' hallucinations from a data perspective and mitigating hallucinations through data-driven methods.</b> Feel free to drop an email if you are interested in connecting. üïäüïäüïä





show_portrait: True
portrait_url: assets/images/photos/ssz3.jpg
portrait_caption: >-
  Welcome to my homepage! This photo was taken before I started my research in NLP. Compared to back then, I've gained quite a bit of weight and lost quite a bit of hair :(

education:
- name: Tsinghua University
  logo: assets/images/badges/tsinghua.png
  position: Ph.D. in Computer Science and Technology
  date: Sep. 2024 - Jul. 2028 (expected)
- name: Peking University 
  logo: assets/images/badges/PKU_red.png
  position: M.S. in Software Engineering
  date: Sep. 2021 - Jul. 2024
- name: Yunnan University 
  logo: assets/images/badges/ynu.png
  position: "B.S. in Information Security (Rank: 1/300+)"
  date: Sep. 2017 - Jul. 2021
  

experience:
- name: DeepLang AI
  logo: assets/images/badges/deeplang.png
  position: Research Intern
  date: Apr. 2024 - Now
- name: Alibaba DAMO Academy
  logo: assets/images/badges/ali.png
  position: Research Intern
  date: Jun. 2022 - Jun. 2023
- name: SenseTime Research
  logo: assets/images/badges/sensetime.png
  position: Research Intern
  date: Jul. 2021 -	Feb. 2022


awards:
- name: Merit Student
  date: 2022
- name: "Top 10 Outstanding Students Nomination Award (Ranked 1st)"
  date: 2020
- name: First-Class Scholarship
  date: 2020
- name: National Scholarship
  date: 2019
- name: Provincial Scholarship
  date: 2018
- name: Provincial Merit Student
  date: 2018
  
service:
- name: "NLP Research Communities: Reviewer of ACL, EMNLP, NAACL, COLING, and TASLP"
- name: "ML Research Communities: Reviewer of NeurIPS, ICLR, and ICML"
- name: "CV Research Communities: Reviewer of ICCV"
- name: I am also a member of the <a href="https://huggingface.co/birdsql">BIRD team</a>, led by the talent researcher Jinyang Li, which drives the development of text-to-SQL for real-world database applications

